# @package _global_
defaults:
  - /experiment/pile/base.yaml

dataset:
  max_length: 4096

model:
  _name_: lm
  d_model: 864
  n_layer: 18
  d_inner: ${eval:2*${.d_model}}
  vocab_size: 50257
  resid_dropout: 0.0
  embed_dropout: 0.1
  layer:
    _name_: hyena-flashfft
    emb_dim: 33 
    filter_order: 64 
    local_order: 3
    l_max: ${dataset.max_length}
    modulate: True
    w: 14
    lr: ${optimizer.lr}
    lr_pos_emb: ${optimizer.lr}
  fused_mlp: True
  residual_in_fp32: True
  pad_vocab_size_multiple: 8
  use_flashfftconv: True

batch_size: 8
dtype: half
