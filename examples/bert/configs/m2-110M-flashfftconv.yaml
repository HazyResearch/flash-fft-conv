max_seq_len: 128
batch_size: 128
dtype: half

model:
  name: bert
  pretrained_model_name: bert-base-uncased
  tokenizer_name: bert-base-uncased
  model_config:
    num_attention_heads: 12 
    num_hidden_layers: 12 
    attention_probs_dropout_prob: 0.0 
    max_position_embeddings: 128

    monarch_mixer_sequence_mixing: True
    use_flashfftconv: True
    inference_mode: True
    long_conv_l_max: 128
    hyena_w: 10
    hyena_wd: 0.1
    hyena_emb_dim: 5
    hyena_filter_order: 128

    bidirectional: True
    residual_long_conv: False

    use_glu_mlp: False
    use_monarch_mlp: False
    monarch_mlp_nblocks: 4
    use_positional_encodings: True